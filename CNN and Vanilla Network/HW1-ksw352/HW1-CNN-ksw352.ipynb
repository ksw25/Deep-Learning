{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lmspyO_Qu5B"
   },
   "source": [
    "## <b> CIFAR-10 CNN using Pytorch <b>\n",
    "## Submitted by :-\n",
    "## Namit Mohale - nm3191\n",
    "## Karanpreet Singh Wadhwa - ksw352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRfEslHuBPuQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FMotFjA6BPuo",
    "outputId": "f3f2e7f5-ea65-425e-b533-ca2acded1c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4S4MfX5qBPvu"
   },
   "outputs": [],
   "source": [
    "class CifarCNN(nn.Module):\n",
    "    def __init__(self, dim1=3, dim2=64, kernel_size1=5, kernel_size2=3, dim3=128, dim4=256, dim5=1024, dim6=512, d_pool=2, n_classes=10):\n",
    "        super(CifarCNN, self).__init__()\n",
    "        \n",
    "        self.dim4 = dim4\n",
    "        self.kernel_size = kernel_size2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(dim1, dim2, kernel_size1, padding = 2, bias = True)\n",
    "        self.conv2 = nn.Conv2d(dim2, dim3, kernel_size2, padding = 1, bias = True)\n",
    "        self.conv3 = nn.Conv2d(dim3, dim4, kernel_size2, padding = 1, bias = True)\n",
    "        self.pool = nn.MaxPool2d(d_pool, d_pool)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(dim4 * 8 * 8, dim5)\n",
    "        self.fc2 = nn.Linear(dim5, dim6)\n",
    "        self.fc3 = nn.Linear(dim6, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #forward pass\n",
    "        #x is the input\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.dropout(self.conv3(x)))\n",
    "        x = x.view(-1, self.dim4 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = F.dropout(F.relu(self.fc2(x)), training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        # x = F.softmax(self.fc3(x), dim=0)\n",
    "        return x\n",
    "\n",
    "net = CifarCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model :\n",
    "1. Convolutes 32x32x3 sample with 64 5x5 filter to 32x32x64\n",
    "2. Applies ReLu\n",
    "3. Takes a 2x2 maxpool and make it 16x16x64 sample\n",
    "4. Convolutes 16x16x64 sample with 128 3x3 filter to 16x16x128\n",
    "5. Applies ReLu\n",
    "6. Takes a 2x2 maxpool and make it 8x8x128 sample\n",
    "7. Convolutes 8x8x128 sample with 256 3x3 filter to 8x8x256\n",
    "8. Applies dropout\n",
    "9. Applies leaky-ReLu activation\n",
    "10. Flattens the sample into a 9216 long layer\n",
    "11. Applies fully connections with the first hidden layer with 1024 nodes followed by a ReLu\n",
    "12. Another fully connected hidden layer with 512 nodes followed by a ReLu\n",
    "13. Maps this to final output layer with 10 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model has been programmed on Google Colab and runs on GPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1332
    },
    "colab_type": "code",
    "id": "YRRwJxO9BPv-",
    "outputId": "1d534597-6408-4c69-942c-ba0cf1fa8112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 1.774\n",
      "[1,  1000] loss: 1.459\n",
      "Accuracy :  41.104\n",
      "[2,   500] loss: 1.319\n",
      "[2,  1000] loss: 1.211\n",
      "Accuracy :  54.934\n",
      "[3,   500] loss: 1.105\n",
      "[3,  1000] loss: 1.061\n",
      "Accuracy :  61.572\n",
      "[4,   500] loss: 0.968\n",
      "[4,  1000] loss: 0.944\n",
      "Accuracy :  66.21\n",
      "[5,   500] loss: 0.864\n",
      "[5,  1000] loss: 0.845\n",
      "Accuracy :  69.984\n",
      "[6,   500] loss: 0.782\n",
      "[6,  1000] loss: 0.763\n",
      "Accuracy :  72.906\n",
      "[7,   500] loss: 0.697\n",
      "[7,  1000] loss: 0.699\n",
      "Accuracy :  75.592\n",
      "[8,   500] loss: 0.629\n",
      "[8,  1000] loss: 0.628\n",
      "Accuracy :  78.204\n",
      "[9,   500] loss: 0.550\n",
      "[9,  1000] loss: 0.555\n",
      "Accuracy :  80.76\n",
      "[10,   500] loss: 0.483\n",
      "[10,  1000] loss: 0.493\n",
      "Accuracy :  82.924\n",
      "[11,   500] loss: 0.420\n",
      "[11,  1000] loss: 0.423\n",
      "Accuracy :  85.428\n",
      "[12,   500] loss: 0.355\n",
      "[12,  1000] loss: 0.361\n",
      "Accuracy :  87.45\n",
      "[13,   500] loss: 0.306\n",
      "[13,  1000] loss: 0.305\n",
      "Accuracy :  89.538\n",
      "[14,   500] loss: 0.246\n",
      "[14,  1000] loss: 0.258\n",
      "Accuracy :  91.332\n",
      "[15,   500] loss: 0.202\n",
      "[15,  1000] loss: 0.211\n",
      "Accuracy :  93.014\n",
      "[16,   500] loss: 0.165\n",
      "[16,  1000] loss: 0.177\n",
      "Accuracy :  94.276\n",
      "[17,   500] loss: 0.137\n",
      "[17,  1000] loss: 0.153\n",
      "Accuracy :  95.032\n",
      "[18,   500] loss: 0.119\n",
      "[18,  1000] loss: 0.127\n",
      "Accuracy :  95.862\n",
      "[19,   500] loss: 0.101\n",
      "[19,  1000] loss: 0.112\n",
      "Accuracy :  96.364\n",
      "[20,   500] loss: 0.089\n",
      "[20,  1000] loss: 0.093\n",
      "Accuracy :  97.0\n",
      "[21,   500] loss: 0.077\n",
      "[21,  1000] loss: 0.086\n",
      "Accuracy :  97.324\n",
      "[22,   500] loss: 0.073\n",
      "[22,  1000] loss: 0.077\n",
      "Accuracy :  97.46\n",
      "[23,   500] loss: 0.066\n",
      "[23,  1000] loss: 0.069\n",
      "Accuracy :  97.722\n",
      "[24,   500] loss: 0.058\n",
      "[24,  1000] loss: 0.067\n",
      "Accuracy :  97.954\n",
      "[25,   500] loss: 0.053\n",
      "[25,  1000] loss: 0.058\n",
      "Accuracy :  98.128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(25): # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        predicted = predicted.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # predictions = outputs.data.max(1)[1]\n",
    "        \n",
    "        # accuracy = np.sum(predictions.cpu().numpy()==labels.cpu().numpy())/50*100\n",
    "        if i % 500 == 499:    # print every 6000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Accuracy : ', 100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frIuEbyRQrx9"
   },
   "source": [
    "Accuracy on Training set - 98.128%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jqsXaskKBPwS",
    "outputId": "cb255fc4-e59c-4564-b19c-7bb827321984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74 %\n"
     ]
    }
   ],
   "source": [
    "correct_test = 0\n",
    "total_test = 0\n",
    "final_test_output = []\n",
    "\n",
    "net.cuda()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted_test = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        predicted_test = predicted_test.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        correct_test += (predicted_test == labels).sum().item()\n",
    "        for item in predicted:\n",
    "            final_test_output.append(item.item())\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct_test / total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines below are just to save the file to system from google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkRl5EbCQpg2"
   },
   "outputs": [],
   "source": [
    "final_test_np = np.asarray(final_test_output)\n",
    "filename = \"HW1_Part2_nm3191_ksw352\"\n",
    "np.save(filename, final_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "gUn3y-mNZ_ZP",
    "outputId": "a7d6dcd2-3572-4054-c114-633b30d80dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  HW1_Part2_nm3191_ksw352.npy  sample_data\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqYV014Vbmnl"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('HW1_Part2_nm3191_ksw352.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network model described above, as we believe, is the perfect combination of complexity, accuracy and time spent. The network was getting better accuracy on the test set with a more complex network but was taking a longer time and other system resources. We believe that it is not advisable to to have a high complexity and longer time taken for training such a model on CPU. Hence we decided to settle for a little low accuracy but a model that would run efficiently even on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning began with changing the optimization function. Adam Optimizer was used to check the result on accuracy. Other than this, neurons were increased upto 2048 in one hidden layer but this only turned the training slow without having a substantial impact on the error rate. Various settings of learning rate and momentum were used and then we decided on one which was giving the maximum accuracy score. Finally, a dropout regularization function was used to avoid overfitting and it yielded better results as accuracy went up to mid 70s. The accuracy also tended to improve when we were training the network for more than 25 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled7.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
